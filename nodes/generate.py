"""
Node GENERATE - G√©n√©ration de r√©ponse avec historique conversationnel (utilisant MessagesState)
"""

"""
Node GENERATE - Response generation with conversational history (usingMessagesState)
"""
import os
from typing import Any
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

def generate(state: dict[str, Any]) -> dict[str, Any]:
    """
    G√©n√®re une r√©ponse en utilisant les documents r√©cup√©r√©s et l'historique conversationnel.
    Utilise MessagesState pour la gestion automatique de l'historique via le checkpointer.
    
    Processus:
    1. R√©cup√®re l'historique conversationnel depuis state["messages"]
    2. Formate le contexte avec les documents
    3. G√©n√®re la r√©ponse avec le LLM
    4. Retourne un AIMessage qui sera automatiquement ajout√© √† l'historique
    
    Args:
        state: GraphState contenant messages et documents
        
    Returns:
        dict avec messages (AIMessage de r√©ponse)
    """
    print("\n--- NODE: GENERATE ---")
    
    messages = state.get("messages", [])
    documents = state.get("documents", [])
    
    #  V√âRIFIER SI QUESTION HORS-SUJET
    is_valid_domain = state.get("is_valid_domain", True)
    domain_check_message = state.get("domain_check_message", "")
    
    if not is_valid_domain and domain_check_message:
        # Retourer directement le message de refus
        print(" Question hors-sujet - Envoi du message de refus")
        ai_message = AIMessage(content=domain_check_message)
        return {
            "messages": [ai_message],
            "generation": domain_check_message
        }
    
    print(f"Historique: {len(messages)} messages")
    print(f"Documents: {len(documents)} documents")
    
    # Configuration
    llm_model = os.getenv("LLM_MODEL", "gpt-4.1-nano")
    llm_temperature = float(os.getenv("LLM_TEMPERATURE", "0.7"))
    
    # Formater le contexte √† partir des documents
    if documents:
        context_parts = []
        for i, doc in enumerate(documents, 1):
            content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
            metadata = doc.metadata if hasattr(doc, 'metadata') else {}
            source = metadata.get('source', metadata.get('url', 'Unknown'))
            context_parts.append(f"Document {i} (Source: {source}):\n{content}")
        context = "\n\n".join(context_parts)
        print(f" Contexte format√©: {len(context)} caract√®res")
    else:
        context = "Aucun document disponible."
        print(" Pas de documents pour le contexte")
    
    # System message avec instructions PRIORIT√â BASE DE CONNAISSANCES
    system_message = SystemMessage(content=f"""Tu es **Dagan**, assistant virtuel pour les citoyens togolais üáπüá¨

**R√àGLE ABSOLUE - Priorit√© des sources :**
1. **BASE DE CONNAISSANCES (documents officiels)** = SOURCE PRINCIPALE
2. **Recherche web (sites officiels .gouv.tg)** = Compl√©ment si n√©cessaire
3. **JAMAIS** de connaissances g√©n√©rales sans v√©rification

**Contexte disponible:**
{context}

**Instructions de r√©ponse :**
- ‚úÖ Utilise UNIQUEMENT les informations du contexte ci-dessus
- ‚úÖ Cite TOUJOURS les sources officielles (URLs)
- ‚úÖ Ton amical et accessible (tutoiement, √©mojis üòä)
- ‚úÖ D√©compose les proc√©dures en √©tapes num√©rot√©es
- ‚ùå NE JAMAIS inventer ou supposer des informations

** SI LA QUESTION EST TROP VAGUE :**
Si la question manque d'informations pour donner une r√©ponse pr√©cise, demande des pr√©cisions :
- "Peux-tu pr√©ciser... ?"
- "S'agit-il de... ?"
- "Quelle est ta situation exacte ?"

**Exemples de questions n√©cessitant des pr√©cisions :**
- "Comment obtenir un document ?" ‚Üí Demande : "Quel document exactement ? (passeport, carte d'identit√©, acte de naissance...)"
- "Je veux faire une demande" ‚Üí Demande : "Quelle demande souhaites-tu faire ?"
- "Quelles sont les proc√©dures ?" ‚Üí Demande : "Quelle proc√©dure t'int√©resse ? (mariage, divorce, cr√©ation d'entreprise...)"

**Format de r√©ponse quand INFO COMPL√àTE :**
[R√©ponse claire et structur√©e]

**Sources :**
-  [Nom de la source] (URL)

**Exemple :**
"Pour obtenir un certificat de nationalit√© togolaise, voici les documents n√©cessaires :

1. Acte de naissance original
2. Photocopie de la carte d'identit√©
3. ...

**Sources :**
-  Service Public du Togo (https://service-public.gouv.tg/...)"
""")
    
    # Construire les messages pour le LLM
    # Historique conversationnel + system message avec contexte
    conversation_messages = [system_message] + list(messages)
    
    print(f" Messages construits: {len(conversation_messages)} au total")
    
    # Initialiser le LLM
    try:
        llm = ChatOpenAI(
            model=llm_model,
            temperature=llm_temperature,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        print(f" LLM initialis√©: {llm_model} (temp√©rature: {llm_temperature})")
        
        # G√©n√©rer la r√©ponse
        response = llm.invoke(conversation_messages)
        generation = response.content
        print(f" R√©ponse g√©n√©r√©e: {len(generation)} caract√®res")
        
        # Retourner un AIMessage qui sera ajout√© automatiquement √† l'historique
        # par le checkpointer de LangGraph
        ai_message = AIMessage(content=generation)
        
        print("--- FIN NODE: GENERATE ---\n")
        
        # Retourner le message ET stocker dans generation pour compatibilit√©
        return {
            "messages": [ai_message],
            "generation": generation
        }
        
    except Exception as e:
        print(f" Erreur g√©n√©ration LLM: {e}")
        error_message = AIMessage(content=f"Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}")
        return {
            "messages": [error_message],
            "generation": f"Erreur: {str(e)}"
        }
